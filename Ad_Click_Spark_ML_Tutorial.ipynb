{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.calstatela.edu/centers/hipic\"><img align=\"left\" src=\"https://avatars2.githubusercontent.com/u/4156894?v=3&s=100\"><image/>\n",
    "</a>\n",
    "<img align=\"right\" alt=\"California State University, Los Angeles\" src=\"http://www.calstatela.edu/sites/default/files/groups/California%20State%20University%2C%20Los%20Angeles/master_logo_full_color_horizontal_centered.svg\" style=\"width: 360px;\"/>\n",
    "\n",
    "# CIS5560 Term Project Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Authors: Hai Anh Le, Neha Gupta, Maria Boldina\n",
    "\n",
    "#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n",
    "\n",
    "#### Date: 05/18/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Spark SQL and Spark ML Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import date_format\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Source Data\n",
    "The data for this exercise is provided as a CSV file containing details of users click. The data includes specific characteristics for each user, as well as a column indicating how many user download the app or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampleSchema = StructType([\n",
    "  StructField(\"ip\", IntegerType(), False),\n",
    "  StructField(\"app\", IntegerType(), False),\n",
    "  StructField(\"device\", IntegerType(), False),\n",
    "  StructField(\"os\", IntegerType(), False),\n",
    "  StructField(\"channel\", IntegerType(), False),\n",
    "  StructField(\"clicktime\", TimestampType (), False),\n",
    "  StructField(\"attributed\", TimestampType(), False),\n",
    "  StructField(\"is_attributed\", IntegerType(), False),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read csv file from DBFS (Databricks File Systems)\n",
    "\n",
    "1. After train_sample_1G.csv file is added to the data of the left frame, create a table using the UI, especially, \"Upload File\"\n",
    "2. Click \"Preview Table to view the table\" and Select the option as train_sample_1G.csv has a header as the first row: \"First line is header\"\n",
    "3. Change the data type of the table columns as shown in train_sampleSchema of the above cell\n",
    "4. When you click on create table button, remember the table name, for example, _train_sample_1G_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls /FileStore/tables/train_sample_1G.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe from the table, using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM train_sample_1G_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the counts of 0's and 1's from is_attributed column to check how many users download the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupBy('is_attributed').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you would want to use sampling.\n",
    "train = df #.sampleBy(\"is_attributed\", fractions={0: 0.02, 1: 0.02})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "Most modeling begins with exhaustive exploration and preparation of the data. In this example, the data has been cleaned for you. You will simply select a subset of columns to use as *features* and create a Boolean *label* field named **label** with the value **1** for users who downloaded the app, or **0** for the users who did not download the app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature -1: Prepare time based feature by extracting day of the week and hour of the day from the click time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_day_of_week = train.withColumn('day_of_week_number',date_format('click_time', 'u').cast('integer')).withColumn('hour_of_day', date_format('click_time', 'H').cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature -2: Prepare feature by grouping clicks by combination of (Ip, Day_of_week_number and Hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpd_by_ip_day_hr = train_with_day_of_week.groupBy('ip', 'day_of_week_number', 'hour_of_day').agg(func.count(func.lit(1)).alias(\"count_by_ip_day_hour\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding Features back to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_1 = train_with_day_of_week.join(grpd_by_ip_day_hr, ['ip','day_of_week_number','hour_of_day'], \"leftouter\")\n",
    "train_with_day_of_week.unpersist()\n",
    "grpd_by_ip_day_hr.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature -3: Prepare feature by grouping clicks by combination of (Ip, App, Operating System, Day_of_week_number and Hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpd_by_ip_app_os_day_hr = train_with_day_of_week.groupBy('ip', 'app','os','day_of_week_number', 'hour_of_day').agg(func.count(func.lit(1)).alias(\"count_by_ip_app_os_day_hour\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding Features back to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_2 = joined_1.join(grpd_by_ip_app_os_day_hr, ['ip','app','os','day_of_week_number','hour_of_day'], \"leftouter\")\n",
    "joined_1.unpersist()\n",
    "grpd_by_ip_app_os_day_hr.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature -4 :  Prepare feature by grouping clicks by combination of (App, Day_of_week_number and Hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpd_by_app_day_hr = train_with_day_of_week.groupBy('app','day_of_week_number', 'hour_of_day').agg(func.count(func.lit(1)).alias(\"grpd_by_app_day_hr\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding Features back to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_3 = joined_2.join(grpd_by_app_day_hr, ['app','day_of_week_number','hour_of_day'], \"leftouter\")\n",
    "joined_2.unpersist()\n",
    "grpd_by_app_day_hr.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature -5 :  Prepare feature by grouping clicks by combination of (Ip, App, Device and Operating System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpd_by_ip_app_dev_os = train_with_day_of_week.groupBy('ip','app','device', 'os').agg(func.count(func.lit(1)).alias(\"grpd_by_ip_app_dev_os\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding Features back to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_4 = joined_3.join(grpd_by_ip_app_dev_os, ['ip','app','device','os'], \"leftouter\")\n",
    "joined_3.unpersist()\n",
    "grpd_by_ip_app_dev_os.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature -6 :  Prepare feature by grouping clicks by combination of (Ip, Device and Operating System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpd_by_ip_dev_os = train_with_day_of_week.groupBy('ip','device', 'os').agg(func.count(func.lit(1)).alias(\"grpd_by_ip_dev_os\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding Features back to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_5 = joined_4.join(grpd_by_ip_dev_os, ['ip','device','os'], \"leftouter\")\n",
    "joined_4.unpersist()\n",
    "grpd_by_ip_dev_os.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Consolidating the data and renaming the target column name (is_attributed) to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joined_5.select('ip', 'device', 'os', 'app', 'day_of_week_number', 'hour_of_day','channel','count_by_ip_day_hour','count_by_ip_app_os_day_hour','grpd_by_app_day_hr','grpd_by_ip_app_dev_os','grpd_by_ip_dev_os', (col(\"is_attributed\").cast(\"Int\").alias(\"label\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data\n",
    "It is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 70% of the data for training, and reserve 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = data.randomSplit([0.7, 0.3],4272)\n",
    "trainingData = splits[0]\n",
    "trainingData.cache()\n",
    "print trainingData.count() # explicitly calling count to cache the training data in memory\n",
    "testingData = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
    "print testingData.count() # explicitly calling count to cache the testing data in memory\n",
    "testingData.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Pipeline\n",
    "A predictive model often requires multiple stages of feature preparation. For example, it is common when using some algorithms to distingish between continuous features (which have a calculable numeric value) and categorical features (which are numeric representations of discrete categories). It is also common to *normalize* continuous numeric features to use a common scale (for example, by scaling all numbers to a proportinal decimal value between 0 and 1).\n",
    "\n",
    "A pipeline consists of a a series of *transformer* and *estimator* stages that typically prepare a DataFrame for\n",
    "modeling and then train a predictive model. In this case, you will create a pipeline with two stages:\n",
    "- A **StringIndexer** estimator that converts string values to indexes for categorical features\n",
    "- A **VectorAssembler** that combines categorical features into a single vector\n",
    "- A **VectorIndexer** that creates indexes for a vector of categorical features\n",
    "- A **VectorAssembler** that creates a vector of continuous numeric features\n",
    "- A **MinMaxScaler** that normalizes continuous numeric features\n",
    "- A **VectorAssembler** that creates a vector of categorical and continuous features\n",
    "- A **DecisionTreeClassifier** that trains a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols = ['ip', 'device', 'os', 'app', 'day_of_week_number', 'hour_of_day','channel','count_by_ip_day_hour','count_by_ip_app_os_day_hour','grpd_by_app_day_hr','grpd_by_ip_app_dev_os','grpd_by_ip_dev_os'], outputCol=\"features\")\n",
    "vi = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning pipeline variables for Decision Tree Classifier\n",
    "\n",
    "The Decision Trees algorithm is popular because it handles categorical data and works out of the box with multiclass classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxDepth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning pipeline variables for Random Forest Classifier\n",
    "Random Forests uses an ensemble of trees to improve model accuracy. You can read more about Random Forest from the classification and regression section of MLlib Programming Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtp = Pipeline(stages=[va, vi, dt])\n",
    "rfp = Pipeline(stages=[va, vi, rf])\n",
    "model = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters\n",
    "You can tune parameters to find the best model for your data. A simple way to do this is to use  **TrainValidationSplit** to evaluate each combination of parameters defined in a **ParameterGrid** against a subset of the training data in order to find the best performing parameters.\n",
    "\n",
    "#### Regularization \n",
    "is a way of avoiding Imbalances in the way that the data is trained against the training data so that the model ends up being over fit to the training data. In other words It works really well with the training data but it doesn't generalize well with other data.\n",
    "That we can use a **regularization parameter** to vary the way that the model balances that way.\n",
    "\n",
    "#### Training ratio of 0.8\n",
    "it's going to use 80% of the the data that it's got in its training set to train the model and then the remaining 20% is going to use to validate the trained model. \n",
    "\n",
    "In **ParamGridBuilder**, all possible combinations are generated from regParam, maxIter, threshold. So it is going to try each combination of the parameters with 80% of the the data to train the model and 20% to to validate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ParamGridBuilder for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [1, 2, 6, 10])\n",
    "             .addGrid(dt.maxBins, [20, 40, 80])\n",
    "             .build())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building and Training Decision Tree Classifier using Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tvs = TrainValidationSplit(estimator=dtp, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\n",
    "model.insert(0, dt_tvs.fit(trainingData))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ParamGridBuilder for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training Random Forest Classifier using Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tvs = TrainValidationSplit(estimator=rfp, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\n",
    "model.insert(1, rf_tvs.fit(trainingData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "Now you're ready to use the **transform** method of the model to generate some predictions. You can use this approach to predict if the app will be downloaded where the label is is-attributed. Also in this case you are using the test data which includes a known true label value, so you can compare the predicted number of clicks which actually led to app being downloaded.\n",
    "\n",
    "\n",
    "%md ### Test the Pipeline Model\n",
    "The model produced by the pipeline is a transformer that will apply all of the stages in the pipeline to a specified DataFrame and apply the trained model to generate predictions. In this case, you will transform the **test** DataFrame using the pipeline to generate label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [] \n",
    "predicted = []\n",
    "for i in range(2):\n",
    "  prediction.insert(i, model[i].transform(testingData))\n",
    "  predicted.insert(i, prediction[i].select(\"features\", \"prediction\", \"probability\", \"trueLabel\"))\n",
    "  predicted[i].show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Truelable and Prediction for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predicted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Truelable and Prediction for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predicted[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Confusion Matrix Metrics: For Decision Tree Classifier\n",
    "Classifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n",
    "- True Positives\n",
    "- True Negatives\n",
    "- False Positives\n",
    "- False Negatives\n",
    "\n",
    "From these core measures, other evaluation metrics such as *precision* and *recall* can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = float(predicted[0].filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "fp = float(predicted[0].filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "tn = float(predicted[0].filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "fn = float(predicted[0].filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "dt_metrics = spark.createDataFrame([\n",
    "    (\"TP\", tp),\n",
    "    (\"FP\", fp),\n",
    "    (\"TN\", tn),\n",
    "    (\"FN\", fn),\n",
    "    (\"Precision\", tp / (tp + fp)),\n",
    "    (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\n",
    "dt_metrics.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Compute Confusion Matrix Metrics of Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dt_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Confusion Matrix Metrics: For Random Forest Classifier\n",
    "Classifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n",
    "- True Positives\n",
    "- True Negatives\n",
    "- False Positives\n",
    "- False Negatives\n",
    "\n",
    "From these core measures, other evaluation metrics such as *precision* and *recall* can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = float(predicted[1].filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "fp = float(predicted[1].filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "tn = float(predicted[1].filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "fn = float(predicted[1].filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "rf_metrics = spark.createDataFrame([\n",
    "    (\"TP\", tp),\n",
    "    (\"FP\", fp),\n",
    "    (\"TN\", tn),\n",
    "    (\"FN\", fn),\n",
    "    (\"Precision\", tp / (tp + fp)),\n",
    "    (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\n",
    "rf_metrics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Compute Confusion Matrix Metrics of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Area Under Curve For Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_evaluator =  BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "dt_auc = dtree_evaluator.evaluate(prediction[0])\n",
    "print \"AUC for Descision Tree Classifier  \",\" = \", dt_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Area Under Curve For Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_evaluator =  BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "rf_auc = rf_evaluator.evaluate(prediction[1])\n",
    "print \"AUC for Random Forest Classifier  \",\" = \", rf_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Area Under Curve For Decision Tree Classifier and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.createDataFrame([ \n",
    "  (\"Decision Tree Classifier\", dt_auc), \n",
    "  (\"Random Forest Classifier\", rf_auc)], [\"Algorithm\", \"Area Under Curve\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Evaluator for Calculating Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Root Mean Square Error for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_rmse = evaluator.evaluate(prediction[0])\n",
    "print \"Root Mean Square Error (RMSE) for Decision Tree Classifier:\", dtree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Root Mean Square Error for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rmse = evaluator.evaluate(prediction[1])\n",
    "print \"Root Mean Square Error (RMSE) for Random Forest Classifier:\", rf_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Root Mean Square Error for Decision Tree Classifier and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.createDataFrame([ \n",
    "  (\"Decision Tree Classifier\", dtree_rmse), \n",
    "  (\"Random Forest Classifier\", rf_rmse)], [\"Algorithm\", \"Root Mean Square Error\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. [DataBricks Guide For Spark ML](https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html)\n",
    "2. [DataSet Link for Spark ML tutorial](https://drive.google.com/file/d/1NiR9dYtEMZnWIMAw-FBEvP_MBcjrcRA4/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "Final-Project-5560",
  "notebookId": 1774108064713872
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
